# ğŸ§  Quiz Heroes - Jogo de Perguntas e Respostas com LLM Local via Ollama

Este projeto Ã© um jogo de perguntas e respostas desenvolvido em Java com arquitetura em camadas (MVC). As perguntas sÃ£o geradas em tempo real por um modelo de linguagem (LLM) executado localmente via **Ollama**.

---

## ğŸ“Œ Objetivo

Proporcionar uma experiÃªncia interativa de quiz com perguntas geradas dinamicamente por uma IA local, sem depender da nuvem. O jogo funciona no terminal (CLI), suporta mÃºltiplos jogadores e fornece um relatÃ³rio de desempenho ao final.

---

## âš™ï¸ Requisitos

- **Java 17 ou superior**
- **Git**
- **Ollama** instalado e rodando localmente
- **Modelo `llama3` (ou outro compatÃ­vel) carregado no Ollama**
- Acesso Ã  internet para o primeiro download do modelo (apÃ³s isso, roda offline)

---

## ğŸ§­ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/Quiz-Heroes.git
cd Quiz-Heroes
```

> Substitua `seu-usuario` pelo seu nome de usuÃ¡rio no GitHub, se aplicÃ¡vel.

---

### 2. Instale o **Ollama**

- Acesse: https://ollama.com/download
- Instale conforme seu sistema operacional:
  - Windows: ExecutÃ¡vel `.msi`
  - Linux (Debian/Ubuntu):

    ```bash
    curl -fsSL https://ollama.com/install.sh | sh
    ```

  - macOS:

    ```bash
    brew install ollama
    ```

---

### 3. Baixe o modelo de linguagem

Recomenda-se o `llama3` (modelo da Meta):

```bash
ollama run llama3
```

Aguarde o download completo (pode levar vÃ¡rios minutos).

---

### 4. Compile e execute o projeto Java

VocÃª pode usar uma IDE como IntelliJ ou Eclipse, mas via terminal funciona assim:

```bash
javac -d bin -sourcepath src src/Main.java
java -cp bin Main
```

---

## ğŸ§ª Funcionalidades

- Cadastro de mÃºltiplos jogadores
- GeraÃ§Ã£o dinÃ¢mica de perguntas pela LLM local
- Interface via terminal (CLI)
- Controle de pontuaÃ§Ã£o
- RelatÃ³rio final com classificaÃ§Ã£o e desempenho
- SeparaÃ§Ã£o clara entre camada de modelo, controle, visÃ£o e serviÃ§o de IA

---

## ğŸ§  Sobre o Ollama e LLM Local

O [Ollama](https://ollama.com) permite executar modelos de linguagem localmente com seguranÃ§a e privacidade. Este projeto utiliza requisiÃ§Ãµes HTTP para interagir com o Ollama e gerar perguntas de forma contextualizada.

---

## ğŸ§° Estruturas de Dados Utilizadas

- `List`: Armazena alternativas das perguntas
- `Map`: Guarda jogadores e pontuaÃ§Ãµes
- `Queue` (simulada via lista): Controla ordem de jogadores

---

## âœ… Exemplo de ExecuÃ§Ã£o

```bash
Digite o nome do jogador 1: Eric
Digite o nome do jogador 2: Ellen

ğŸ§  Gerando pergunta...
Jogador: Eric
Pergunta: "Qual foi o principal motivo da queda do ImpÃ©rio Romano?"
a) Guerras Civis
b) Crise EconÃ´mica
c) InvasÃµes BÃ¡rbaras
d) Cristianismo
> Sua resposta: c
âœ… Correto!

...

ğŸ‰ Fim da partida! Ranking:
1. Eric - 3 acertos
2. Ellen - 2 acertos
```

---

## ğŸ§ª Testes

Este projeto inclui:

- **Testes unitÃ¡rios**: localizados em `test/unit`, verificam mÃ©todos individuais das classes de serviÃ§o e parser;
- **Testes com stubs**: como `OllamaClientStub` e `QuizCLIStub`, usados para simular a interaÃ§Ã£o com a LLM e a CLI, permitindo testes offline;
- **Testes de integraÃ§Ã£o**: localizados em `test/integration`, validam o comportamento conjunto entre `QuizService`, `QuizCLI` e `OllamaClient`.

Os testes sÃ£o escritos em JUnit e podem ser executados diretamente na IDE ou via terminal.

---

## âœ¨ ContribuiÃ§Ãµes

Sinta-se livre para abrir issues ou pull requests para melhorar o jogo.

---

## ğŸ“œ LicenÃ§a

Este projeto Ã© de uso acadÃªmico e pessoal. Direitos reservados a Eric Diego Matozo GonÃ§alves.
